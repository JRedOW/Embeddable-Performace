\section{Introduction}
The paper is meant to be a resource for developers needing to use an embeddable language, but unsure on which to use. While languages such Python and JavaScript have long remained supreme (with lua in gaming)\cite{trend:jslua}, newer languages have entered the scene.

\subsection{Focus}
Embeddable languages are created first and foremost as an easy way to modify a program's ability at run time. By focussing on end-user customization, these languages must balance performance, code readability, and features. Testing code readability is impossible as readability is an extremely subjective measurement, influenced by programer's tastes, needs, and experience with similar languages. While testing language features seems to open a path to study, the field of computer languages is constantly evolving, making trends have a very disproportionate impact on measurements. There is also no objectivity better or worse syntax (despite what StackOverflow says), which is compounded by the differing syntactical needs of different programming paradigms.

\subsection{Performance Testing}
This leaves performance, which can be objectively measured at the simplest levels of a language. Performance of an embeddable language, at a general level, can be put into two fields, embedding performance and language performance. Embeddable performance covers the instancing and overhead of using the embeddable language in a software. Language performance measures the actual execution time of complex processes that the embeddable language must execute. By comparing these two fields, we can get a semi-comprehensive image of the language's performance.

\subsection{Existing Results}
The current environment is interestingly bare. There does exist extensive testing within JavaScript engines, though these only compare different systems to execute a single language, JavaScript\cite{arewefastyet}. Along with that, there are benchmarks that exist, though they are often created by the creators of a competitor language (a conflict of interest) or cover only a few common languages, and these benchmarks are almost always hard to replicate. These benchmarks are not bad research though, this paper's methods for testing embeddable work were heavily inspired by Takayuki Sato's earlier work\cite{embench}.
